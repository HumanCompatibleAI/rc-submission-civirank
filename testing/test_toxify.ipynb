{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from detoxify import Detoxify\n",
    "from civirank import parsers, analyzers\n",
    "import json\n",
    "%load_ext line_profiler\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data\n",
    "fname = \"twitter_test.json\"\n",
    "with open(fname, \"r\") as fin:\n",
    "    sample_data_twitter = json.load(fin)\n",
    "fname = \"reddit_test.json\"\n",
    "with open(fname, \"r\") as fin:\n",
    "    sample_data_reddit = json.load(fin)\n",
    "fname = \"facebook_test.json\"\n",
    "with open(fname, \"r\") as fin:\n",
    "    sample_data_facebook = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicityAnalyzer():\n",
    "    def __init__(self, model_type='original', batch_size=8):\n",
    "        # Initialize the Detoxify model with the specified model type\n",
    "        self.detoxify_model = Detoxify(model_type, device='cuda')\n",
    "        # adds batch size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_toxicity_scores(self, text):\n",
    "        \"\"\" Analyze the given text and return toxicity scores \"\"\"\n",
    "        assert type(text) in [str, pd.core.frame.DataFrame]\n",
    "        if type(text) == str:\n",
    "            results = self.detoxify_model.predict(text)\n",
    "            return results['toxicity']\n",
    "        else:\n",
    "            scores = []\n",
    "            for i in range(0, len(text), self.batch_size):\n",
    "                batch = text[\"text\"].iloc[i:i+self.batch_size].tolist()\n",
    "                results = self.detoxify_model.predict(batch)\n",
    "                scores.extend(results['toxicity'])\n",
    "            return scores\n",
    "\n",
    "\n",
    "class ToxicityAnalyzerONNX():\n",
    "    def __init__(self, model_name=\"protectai/unbiased-toxic-roberta-onnx\", file_name='model.onnx', batch_size=8, max_length=512):\n",
    "        # Initialize the ONNX model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = ORTModelForSequenceClassification.from_pretrained(model_name, file_name=file_name, provider=\"CUDAExecutionProvider\")\n",
    "        self.classifier = pipeline(\n",
    "            task=\"text-classification\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            top_k=None,  # Use top_k=None to get all scores\n",
    "        )\n",
    "        # Adds batch size\n",
    "        self.batch_size = batch_size\n",
    "        # Set max length for input sequences\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def get_toxicity_scores(self, text):\n",
    "        \"\"\" Analyze the given text and return toxicity scores \"\"\"\n",
    "        assert type(text) in [str, pd.core.frame.DataFrame]\n",
    "\n",
    "        def extract_toxicity_score(predictions):\n",
    "            for pred in predictions:\n",
    "                if pred['label'] == 'toxicity':\n",
    "                    return pred['score']\n",
    "            raise ValueError(\"Toxicity label not found in predictions\")\n",
    "\n",
    "        def truncate_text(text, max_length):\n",
    "            # Tokenize and truncate the text to the max length\n",
    "            tokens = self.tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            return self.tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "\n",
    "        if type(text) == str:\n",
    "            truncated_text = truncate_text(text, self.max_length)\n",
    "            results = self.classifier(truncated_text)\n",
    "            return extract_toxicity_score(results[0])\n",
    "        else:\n",
    "            scores = []\n",
    "            for i in range(0, len(text), self.batch_size):\n",
    "                batch = text[\"text\"].iloc[i:i+self.batch_size].tolist()\n",
    "                truncated_batch = [truncate_text(t, self.max_length) for t in batch]\n",
    "                results = self.classifier(truncated_batch)\n",
    "                batch_scores = [extract_toxicity_score(result) for result in results]\n",
    "                scores.extend(batch_scores)\n",
    "            return scores\n",
    "\n",
    "class ToxicityAnalyzerONNX2():\n",
    "    def __init__(self, model_name=\"protectai/unbiased-toxic-roberta-onnx\", file_name='model.onnx', batch_size=8, max_length=512):\n",
    "        # Initialize the ONNX model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = ORTModelForSequenceClassification.from_pretrained(model_name, file_name=file_name,provider=\"CUDAExecutionProvider\")\n",
    "        self.classifier = pipeline(\n",
    "            task=\"text-classification\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            top_k=None,  # Use top_k=None to get all scores\n",
    "        )\n",
    "        # Adds batch size\n",
    "        self.batch_size = batch_size\n",
    "        # Set max length for input sequences\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def get_toxicity_scores(self, text):\n",
    "        \"\"\" Analyze the given text and return toxicity scores \"\"\"\n",
    "        assert type(text) in [str, pd.core.frame.DataFrame]\n",
    "\n",
    "        def extract_toxicity_score(predictions):\n",
    "            for pred in predictions:\n",
    "                if pred['label'].lower() in ['toxic', 'toxicity']:\n",
    "                    return pred['score']\n",
    "            return 0.0\n",
    "\n",
    "        def truncate_text(text, max_length):\n",
    "            # Tokenize and truncate the text to the max length\n",
    "            tokens = self.tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            return self.tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "\n",
    "        if type(text) == str:\n",
    "            truncated_text = truncate_text(text, self.max_length)\n",
    "            results = self.classifier(truncated_text)\n",
    "            return extract_toxicity_score(results[0])\n",
    "        else:\n",
    "            # Prepare the dataset\n",
    "            text_list = text[\"text\"].tolist()\n",
    "            truncated_texts = [truncate_text(t, self.max_length) for t in text_list]\n",
    "            dataset = Dataset.from_dict({\"text\": truncated_texts})\n",
    "            \n",
    "            # Process the dataset in batches\n",
    "            scores = []\n",
    "            for i in range(0, len(dataset), self.batch_size):\n",
    "                batch = dataset.select(range(i, min(i + self.batch_size, len(dataset))))\n",
    "                results = self.classifier(batch[\"text\"])\n",
    "                batch_scores = [extract_toxicity_score(result) for result in results]\n",
    "                scores.extend(batch_scores)\n",
    "            return scores\n",
    "\n",
    "class ToxicityAnalyzerONNX3():\n",
    "    def __init__(self, model_name=\"protectai/unbiased-toxic-roberta-onnx\", batch_size=8, file_name='model.onnx', gpu_id=0):\n",
    "        # Initialize the ONNX model and tokenizer with the specified model name\n",
    "        self.model = ORTModelForSequenceClassification.from_pretrained(model_name, file_name=file_name, provider=\"CUDAExecutionProvider\", provider_options={'device_id': gpu_id})\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Find the index of the 'toxicity' label\n",
    "        self.toxicity_index = None\n",
    "        for idx, label in self.model.config.id2label.items():\n",
    "            if label.lower() == 'toxicity':\n",
    "                self.toxicity_index = idx\n",
    "                break\n",
    "        if self.toxicity_index is None:\n",
    "            raise ValueError(\"Toxicity label not found in model's id2label mapping.\")\n",
    "\n",
    "    def classify_texts(self, texts):\n",
    "        \"\"\" Tokenize and classify a batch of texts \"\"\"\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        probabilities = torch.sigmoid(outputs.logits)\n",
    "\n",
    "        batch_results = []\n",
    "        for prob in probabilities:\n",
    "            result = prob[self.toxicity_index].item()  # Get the probability for the 'toxicity' label\n",
    "            batch_results.append(result)\n",
    "\n",
    "        return batch_results\n",
    "\n",
    "    def get_toxicity_scores(self, text):\n",
    "        \"\"\" Analyze the given text or DataFrame and return toxicity scores \"\"\"\n",
    "        assert isinstance(text, (str, pd.DataFrame)), \"Input should be either a string or a DataFrame\"\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            results = self.classify_texts([text])\n",
    "            return results[0]  # Return the score for the single input string\n",
    "        else:\n",
    "            results = []\n",
    "            for start in range(0, len(text), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_texts = text[\"text\"].iloc[start:end].tolist()\n",
    "                batch_results = self.classify_texts(batch_texts)\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            results_df = pd.DataFrame(results, index=text.index[:len(results)], columns=[\"toxicity\"])\n",
    "            return results_df\n",
    "\n",
    "class ToxicityAnalyzerONNX3Q():\n",
    "    def __init__(self, model_name=\"protectai/unbiased-toxic-roberta-onnx\", batch_size=8, file_name='model_quantized.onnx', gpu_id=0):\n",
    "        # Initialize the ONNX model and tokenizer with the specified model name\n",
    "        self.model = ORTModelForSequenceClassification.from_pretrained(model_name, file_name=file_name, provider=\"CPUExecutionProvider\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Find the index of the 'toxicity' label\n",
    "        self.toxicity_index = None\n",
    "        for idx, label in self.model.config.id2label.items():\n",
    "            if label.lower() == 'toxicity':\n",
    "                self.toxicity_index = idx\n",
    "                break\n",
    "        if self.toxicity_index is None:\n",
    "            raise ValueError(\"Toxicity label not found in model's id2label mapping.\")\n",
    "\n",
    "    def classify_texts(self, texts):\n",
    "        \"\"\" Tokenize and classify a batch of texts \"\"\"\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        probabilities = torch.sigmoid(outputs.logits)\n",
    "\n",
    "        batch_results = []\n",
    "        for prob in probabilities:\n",
    "            result = prob[self.toxicity_index].item()  # Get the probability for the 'toxicity' label\n",
    "            batch_results.append(result)\n",
    "\n",
    "        return batch_results\n",
    "\n",
    "    def get_toxicity_scores(self, text):\n",
    "        \"\"\" Analyze the given text or DataFrame and return toxicity scores \"\"\"\n",
    "        assert isinstance(text, (str, pd.DataFrame)), \"Input should be either a string or a DataFrame\"\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            results = self.classify_texts([text])\n",
    "            return results[0]  # Return the score for the single input string\n",
    "        else:\n",
    "            results = []\n",
    "            for start in range(0, len(text), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_texts = text[\"text\"].iloc[start:end].tolist()\n",
    "                batch_results = self.classify_texts(batch_texts)\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            results_df = pd.DataFrame(results, index=text.index[:len(results)], columns=[\"toxicity\"])\n",
    "            return results_df\n",
    "\n",
    "class ToxicityAnalyzerONNX3QRT():\n",
    "    def __init__(self, model_name=\"protectai/unbiased-toxic-roberta-onnx\", batch_size=8, file_name='model_quantized.onnx', gpu_id=0):\n",
    "        # Initialize the ONNX model and tokenizer with the specified model name\n",
    "        self.model = ORTModelForSequenceClassification.from_pretrained(model_name, file_name=file_name, provider=\"TensorrtExecutionProvider\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Find the index of the 'toxicity' label\n",
    "        self.toxicity_index = None\n",
    "        for idx, label in self.model.config.id2label.items():\n",
    "            if label.lower() == 'toxicity':\n",
    "                self.toxicity_index = idx\n",
    "                break\n",
    "        if self.toxicity_index is None:\n",
    "            raise ValueError(\"Toxicity label not found in model's id2label mapping.\")\n",
    "\n",
    "    def classify_texts(self, texts):\n",
    "        \"\"\" Tokenize and classify a batch of texts \"\"\"\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        probabilities = torch.sigmoid(outputs.logits)\n",
    "\n",
    "        batch_results = []\n",
    "        for prob in probabilities:\n",
    "            result = prob[self.toxicity_index].item()  # Get the probability for the 'toxicity' label\n",
    "            batch_results.append(result)\n",
    "\n",
    "        return batch_results\n",
    "\n",
    "    def get_toxicity_scores(self, text):\n",
    "        \"\"\" Analyze the given text or DataFrame and return toxicity scores \"\"\"\n",
    "        assert isinstance(text, (str, pd.DataFrame)), \"Input should be either a string or a DataFrame\"\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            results = self.classify_texts([text])\n",
    "            return results[0]  # Return the score for the single input string\n",
    "        else:\n",
    "            results = []\n",
    "            for start in range(0, len(text), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_texts = text[\"text\"].iloc[start:end].tolist()\n",
    "                batch_results = self.classify_texts(batch_texts)\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            results_df = pd.DataFrame(results, index=text.index[:len(results)], columns=[\"toxicity\"])\n",
    "            return results_df\n",
    "\n",
    "def parse_data(dataset):\n",
    "\n",
    "    LanguageAnalyzer = analyzers.LanguageAnalyzer()\n",
    "\n",
    "    platform = dataset[\"session\"][\"platform\"]\n",
    "\n",
    "    for i in range(len(dataset[\"items\"])):\n",
    "        dataset['items'][i]['lang'] = LanguageAnalyzer.detect_language(dataset['items'][i]['text'].replace('\\n', ' '))\n",
    "\n",
    "    if platform == \"twitter\":\n",
    "        posts = parsers.parse_twitter_posts(dataset[\"items\"])\n",
    "    elif platform == \"reddit\":\n",
    "        posts = parsers.parse_reddit_posts(dataset[\"items\"])\n",
    "    elif platform == \"facebook\":\n",
    "        posts = parsers.parse_facebook_posts(dataset[\"items\"])\n",
    "\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974874258041382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-06-15 04:25:10.311073860 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 04:25:10.311100490 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "analyzer = ToxicityAnalyzerONNX3()\n",
    "text = 'shut up, you idiot!'\n",
    "print(analyzer.get_toxicity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2024-06-15 04:27:11.895393344 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 04:27:11.895421287 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.4575917720794678\n"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX3()\n",
    "data = parse_data(sample_data_reddit)\n",
    "tic = time.time()\n",
    "v.get_toxicity_scores(data)\n",
    "toc = time.time()\n",
    "print(\"Time taken: \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  3.2566821575164795\n"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX3Q()\n",
    "data = parse_data(sample_data_reddit)\n",
    "tic = time.time()\n",
    "v.get_toxicity_scores(data)\n",
    "toc = time.time()\n",
    "print(\"Time taken: \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  7.788755893707275\n"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX3Q(file_name='model.onnx')\n",
    "data = parse_data(sample_data_reddit)\n",
    "tic = time.time()\n",
    "v.get_toxicity_scores(data)\n",
    "toc = time.time()\n",
    "print(\"Time taken: \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-06-15 04:29:12.068221320 [E:onnxruntime:Default, provider_bridge_ort.cc:1730 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2024-06-15 04:29:12.195785637 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 294 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  4.874242305755615\n"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX3QRT(file_name='model_quantized.onnx')\n",
    "data = parse_data(sample_data_reddit)\n",
    "tic = time.time()\n",
    "v.get_toxicity_scores(data)\n",
    "toc = time.time()\n",
    "print(\"Time taken: \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99506074\n"
     ]
    }
   ],
   "source": [
    "analyzer = ToxicityAnalyzer()\n",
    "text = 'shut up, you idiot!'\n",
    "print(analyzer.get_toxicity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 0.755877 s\n",
      "File: /tmp/ipykernel_454289/4292279429.py\n",
      "Function: get_toxicity_scores at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                               def get_toxicity_scores(self, text):\n",
      "     9                                                   \"\"\" Analyze the given text and return toxicity scores \"\"\"\n",
      "    10         1          0.0      0.0      0.0          assert type(text) in [str, pd.core.frame.DataFrame]\n",
      "    11         1          0.0      0.0      0.0          if type(text) == str:\n",
      "    12                                                       results = self.detoxify_model.predict(text)\n",
      "    13                                                       return results['toxicity']\n",
      "    14                                                   else:\n",
      "    15         1          0.0      0.0      0.0              scores = []\n",
      "    16        21          0.0      0.0      0.0              for i in range(0, len(text), self.batch_size):\n",
      "    17        20          0.0      0.0      0.2                  batch = text[\"text\"].iloc[i:i+self.batch_size].tolist()\n",
      "    18        20          0.8      0.0     99.8                  results = self.detoxify_model.predict(batch)\n",
      "    19        20          0.0      0.0      0.0                  scores.extend(results['toxicity'])\n",
      "    20         1          0.0      0.0      0.0              return scores"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzer()\n",
    "data = parse_data(sample_data_reddit)\n",
    "%lprun -u 1 -f v.get_toxicity_scores v.get_toxicity_scores(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6244995594024658\n"
     ]
    }
   ],
   "source": [
    "analyzer = ToxicityAnalyzer()\n",
    "data = parse_data(sample_data_reddit)\n",
    "tic = time.time()\n",
    "analyzer.get_toxicity_scores(data)\n",
    "toc = time.time()\n",
    "print(toc - tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2024-06-15 03:38:35.758374975 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 294 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-06-15 03:38:35.761604234 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 03:38:35.761610035 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 5.84949 s\n",
      "File: /tmp/ipykernel_454289/4292279429.py\n",
      "Function: get_toxicity_scores at line 84\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    84                                               def get_toxicity_scores(self, text):\n",
      "    85                                                   \"\"\" Analyze the given text and return toxicity scores \"\"\"\n",
      "    86         1          0.0      0.0      0.0          assert type(text) in [str, pd.core.frame.DataFrame]\n",
      "    87                                           \n",
      "    88         1          0.0      0.0      0.0          def extract_toxicity_score(predictions):\n",
      "    89                                                       for pred in predictions:\n",
      "    90                                                           if pred['label'].lower() in ['toxic', 'toxicity']:\n",
      "    91                                                               return pred['score']\n",
      "    92                                                       return 0.0\n",
      "    93                                           \n",
      "    94         1          0.0      0.0      0.0          def truncate_text(text, max_length):\n",
      "    95                                                       # Tokenize and truncate the text to the max length\n",
      "    96                                                       tokens = self.tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt')\n",
      "    97                                                       return self.tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
      "    98                                           \n",
      "    99         1          0.0      0.0      0.0          if type(text) == str:\n",
      "   100                                                       truncated_text = truncate_text(text, self.max_length)\n",
      "   101                                                       results = self.classifier(truncated_text)\n",
      "   102                                                       return extract_toxicity_score(results[0])\n",
      "   103                                                   else:\n",
      "   104                                                       # Prepare the dataset\n",
      "   105         1          0.0      0.0      0.0              text_list = text[\"text\"].tolist()\n",
      "   106         1          0.0      0.0      0.5              truncated_texts = [truncate_text(t, self.max_length) for t in text_list]\n",
      "   107         1          0.0      0.0      0.1              dataset = Dataset.from_dict({\"text\": truncated_texts})\n",
      "   108                                                       \n",
      "   109                                                       # Process the dataset in batches\n",
      "   110         1          0.0      0.0      0.0              scores = []\n",
      "   111        21          0.0      0.0      0.0              for i in range(0, len(dataset), self.batch_size):\n",
      "   112        20          0.0      0.0      0.5                  batch = dataset.select(range(i, min(i + self.batch_size, len(dataset))))\n",
      "   113        20          5.8      0.3     98.9                  results = self.classifier(batch[\"text\"])\n",
      "   114        20          0.0      0.0      0.0                  batch_scores = [extract_toxicity_score(result) for result in results]\n",
      "   115        20          0.0      0.0      0.0                  scores.extend(batch_scores)\n",
      "   116         1          0.0      0.0      0.0              return scores"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX2(file_name='model_quantized.onnx')\n",
    "data = parse_data(sample_data_reddit)\n",
    "%lprun -u 1 -f v.get_toxicity_scores v.get_toxicity_scores(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974874258041382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-06-15 03:00:26.144434704 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 03:00:26.144460263 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "analyzer = ToxicityAnalyzerONNX()\n",
    "text = 'shut up, you idiot!'\n",
    "print(analyzer.get_toxicity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2024-06-15 03:05:29.174982030 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 03:05:29.175009432 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 1.20212 s\n",
      "File: /tmp/ipykernel_454289/1449302690.py\n",
      "Function: get_toxicity_scores at line 39\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    39                                               def get_toxicity_scores(self, text):\n",
      "    40                                                   \"\"\" Analyze the given text and return toxicity scores \"\"\"\n",
      "    41         1          0.0      0.0      0.0          assert type(text) in [str, pd.core.frame.DataFrame]\n",
      "    42                                           \n",
      "    43         1          0.0      0.0      0.0          def extract_toxicity_score(predictions):\n",
      "    44                                                       for pred in predictions:\n",
      "    45                                                           if pred['label'] == 'toxicity':\n",
      "    46                                                               return pred['score']\n",
      "    47                                                       raise ValueError(\"Toxicity label not found in predictions\")\n",
      "    48                                           \n",
      "    49         1          0.0      0.0      0.0          def truncate_text(text, max_length):\n",
      "    50                                                       # Tokenize and truncate the text to the max length\n",
      "    51                                                       tokens = self.tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt')\n",
      "    52                                                       return self.tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
      "    53                                           \n",
      "    54         1          0.0      0.0      0.0          if type(text) == str:\n",
      "    55                                                       truncated_text = truncate_text(text, self.max_length)\n",
      "    56                                                       results = self.classifier(truncated_text)\n",
      "    57                                                       return extract_toxicity_score(results[0])\n",
      "    58                                                   else:\n",
      "    59         1          0.0      0.0      0.0              scores = []\n",
      "    60        21          0.0      0.0      0.0              for i in range(0, len(text), self.batch_size):\n",
      "    61        20          0.0      0.0      0.1                  batch = text[\"text\"].iloc[i:i+self.batch_size].tolist()\n",
      "    62        20          0.0      0.0      2.5                  truncated_batch = [truncate_text(t, self.max_length) for t in batch]\n",
      "    63        20          1.2      0.1     97.4                  results = self.classifier(truncated_batch)\n",
      "    64        20          0.0      0.0      0.0                  batch_scores = [extract_toxicity_score(result) for result in results]\n",
      "    65        20          0.0      0.0      0.0                  scores.extend(batch_scores)\n",
      "    66         1          0.0      0.0      0.0              return scores"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX()\n",
    "data = parse_data(sample_data_reddit)\n",
    "%lprun -u 1 -f v.get_toxicity_scores v.get_toxicity_scores(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2024-06-15 03:05:33.748742712 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 03:05:33.748769082 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0257039070129395\n"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX()\n",
    "data = parse_data(sample_data_reddit)\n",
    "tic = time.time()\n",
    "v.get_toxicity_scores(data)\n",
    "toc = time.time()\n",
    "print(toc - tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2024-06-15 03:09:44.733053865 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 03:09:44.733084253 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 1.19457 s\n",
      "File: /tmp/ipykernel_454289/4292279429.py\n",
      "Function: get_toxicity_scores at line 84\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    84                                               def get_toxicity_scores(self, text):\n",
      "    85                                                   \"\"\" Analyze the given text and return toxicity scores \"\"\"\n",
      "    86         1          0.0      0.0      0.0          assert type(text) in [str, pd.core.frame.DataFrame]\n",
      "    87                                           \n",
      "    88         1          0.0      0.0      0.0          def extract_toxicity_score(predictions):\n",
      "    89                                                       for pred in predictions:\n",
      "    90                                                           if pred['label'].lower() in ['toxic', 'toxicity']:\n",
      "    91                                                               return pred['score']\n",
      "    92                                                       return 0.0\n",
      "    93                                           \n",
      "    94         1          0.0      0.0      0.0          def truncate_text(text, max_length):\n",
      "    95                                                       # Tokenize and truncate the text to the max length\n",
      "    96                                                       tokens = self.tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt')\n",
      "    97                                                       return self.tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
      "    98                                           \n",
      "    99         1          0.0      0.0      0.0          if type(text) == str:\n",
      "   100                                                       truncated_text = truncate_text(text, self.max_length)\n",
      "   101                                                       results = self.classifier(truncated_text)\n",
      "   102                                                       return extract_toxicity_score(results[0])\n",
      "   103                                                   else:\n",
      "   104                                                       # Prepare the dataset\n",
      "   105         1          0.0      0.0      0.0              text_list = text[\"text\"].tolist()\n",
      "   106         1          0.0      0.0      2.4              truncated_texts = [truncate_text(t, self.max_length) for t in text_list]\n",
      "   107         1          0.0      0.0      0.3              dataset = Dataset.from_dict({\"text\": truncated_texts})\n",
      "   108                                                       \n",
      "   109                                                       # Process the dataset in batches\n",
      "   110         1          0.0      0.0      0.0              scores = []\n",
      "   111        21          0.0      0.0      0.0              for i in range(0, len(dataset), self.batch_size):\n",
      "   112        20          0.0      0.0      1.5                  batch = dataset.select(range(i, min(i + self.batch_size, len(dataset))))\n",
      "   113        20          1.1      0.1     95.9                  results = self.classifier(batch[\"text\"])\n",
      "   114        20          0.0      0.0      0.0                  batch_scores = [extract_toxicity_score(result) for result in results]\n",
      "   115        20          0.0      0.0      0.0                  scores.extend(batch_scores)\n",
      "   116         1          0.0      0.0      0.0              return scores"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX2()\n",
    "data = parse_data(sample_data_reddit)\n",
    "%lprun -u 1 -f v.get_toxicity_scores v.get_toxicity_scores(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/bench/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2024-06-15 03:09:39.196171842 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-06-15 03:09:39.196220154 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.137446403503418\n"
     ]
    }
   ],
   "source": [
    "v = ToxicityAnalyzerONNX2()\n",
    "data = parse_data(sample_data_reddit)\n",
    "tic = time.time()\n",
    "v.get_toxicity_scores(data)\n",
    "toc = time.time()\n",
    "print(toc - tic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranker311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
