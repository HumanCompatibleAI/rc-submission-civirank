{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/ranker/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/joaopn/software/miniconda3/envs/ranker/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'toxic', 'score': 0.9950607419013977},\n",
       "  {'label': 'severe_toxic', 'score': 0.07963104546070099},\n",
       "  {'label': 'obscene', 'score': 0.8713389039039612},\n",
       "  {'label': 'threat', 'score': 0.001953667961061001},\n",
       "  {'label': 'insult', 'score': 0.9586619138717651},\n",
       "  {'label': 'identity_hate', 'score': 0.014700641855597496}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "detoxify_pipeline = pipeline(\n",
    "     'text-classification', \n",
    "     model='unitary/toxic-bert', \n",
    "     tokenizer='bert-base-uncased', \n",
    "     function_to_apply='sigmoid', \n",
    "     return_all_scores=True\n",
    "     )\n",
    "\n",
    "detoxify_pipeline('shut up, you idiot!')\n",
    "# [[{'label': 'toxic', 'score': 0.9950607419013977}, \n",
    "# {'label': 'severe_toxic', 'score': 0.07963108271360397}, \n",
    "# {'label': 'obscene', 'score': 0.8713390231132507}, \n",
    "# {'label': 'threat', 'score': 0.0019536688923835754}, \n",
    "# {'label': 'insult', 'score': 0.9586619138717651}, \n",
    "# {'label': 'identity_hate', 'score': 0.014700635336339474}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/ranker/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxicity': 0.99506074, 'severe_toxicity': 0.079631045, 'obscene': 0.8713389, 'threat': 0.001953668, 'insult': 0.9586619, 'identity_attack': 0.014700642}\n"
     ]
    }
   ],
   "source": [
    "model = Detoxify(\"original\")\n",
    "res = model.predict('shut up, you idiot!')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99506074 0.07963105 0.8713389  0.00195367 0.9586619  0.01470064]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('unitary/toxic-bert')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('unitary/toxic-bert')\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer('shut up, you idiot!', return_tensors='pt')\n",
    "\n",
    "# Get the model's output\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Apply sigmoid to the outputs\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probabilities = sigmoid(outputs.logits)\n",
    "\n",
    "# Get all scores\n",
    "scores = probabilities.cpu().numpy()\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'toxicity', 'score': 0.9974876642227173}]\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"protectai/unbiased-toxic-roberta-onnx\", file_name=\"model.onnx\")\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"laiyer/unbiased-toxic-roberta-onnx\", file_name=\"model.onnx\")\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "classifier_output = classifier('shut up, you idiot!')\n",
    "print(classifier_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_onnx_batched(model_id, df, field_name, batch_size, gpu_id):\n",
    "    file_name = \"onnx/model.onnx\"\n",
    "\n",
    "    model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=file_name, provider=\"CUDAExecutionProvider\", provider_options={'device_id': gpu_id})\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Function to classify emotions of multiple texts in batched mode and return scores\n",
    "    def classify_texts(texts):\n",
    "        # Tokenize the batch of texts\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        probabilities = torch.sigmoid(outputs.logits)\n",
    "        labels = model.config.id2label  # Adjust if necessary\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        batch_results = []\n",
    "        for prob in probabilities:\n",
    "            result = {labels[i]: prob_item.item() for i, prob_item in enumerate(prob.squeeze())}\n",
    "            batch_results.append(result)\n",
    "            \n",
    "        return batch_results\n",
    "\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'toxicity', 'score': 0.9974876642227173}]\n",
      "Time: 0.011 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "classifier_output = classifier('shut up, you idiot!')\n",
    "print(classifier_output)\n",
    "toc = time.time()\n",
    "print(f\"Time: {toc - tic:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopn/software/miniconda3/envs/ranker/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.017 s\n",
      "{'toxicity': 0.99738663, 'severe_toxicity': 0.0018401984, 'obscene': 0.038010202, 'identity_attack': 0.004118336, 'insult': 0.9938607, 'threat': 0.00080649974, 'sexual_explicit': 0.0021994496}\n"
     ]
    }
   ],
   "source": [
    "model = Detoxify(\"unbiased\")\n",
    "tic = time.time()\n",
    "res = model.predict('shut up, you idiot!')\n",
    "toc = time.time()\n",
    "print(f\"Time: {toc - tic:.3f} s\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
